{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3f45d03-2bb8-417f-a995-be7f06b6baec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando processo de ETL...\n",
      "==================================================\n",
      "Processando 'Amazon Sale Report.csv'...\n",
      "Carregando dados para a tabela: 'cln_amazon_sale_report'...\n",
      "✅ Sucesso! Tabela 'cln_amazon_sale_report' carregada/atualizada no banco de dados.\n",
      "✅ Sucesso! Arquivo limpo salvo em: 'C:\\Users\\wallace.magalhaes\\Desktop\\E-commerce-database\\data\\clean\\cln_amazon_sale_report.csv'\n",
      "\n",
      "Processando 'Cloud Warehouse Compersion Chart.csv'...\n",
      "Carregando dados para a tabela: 'cln_cloud_warehouse_compersion_chart'...\n",
      "✅ Sucesso! Tabela 'cln_cloud_warehouse_compersion_chart' carregada/atualizada no banco de dados.\n",
      "✅ Sucesso! Arquivo limpo salvo em: 'C:\\Users\\wallace.magalhaes\\Desktop\\E-commerce-database\\data\\clean\\cln_cloud_warehouse_compersion_chart.csv'\n",
      "\n",
      "Processando 'Expense IIGF.csv'...\n",
      "Carregando dados para a tabela: 'cln_expense_iigf'...\n",
      "✅ Sucesso! Tabela 'cln_expense_iigf' carregada/atualizada no banco de dados.\n",
      "✅ Sucesso! Arquivo limpo salvo em: 'C:\\Users\\wallace.magalhaes\\Desktop\\E-commerce-database\\data\\clean\\cln_expense_iigf.csv'\n",
      "\n",
      "Processando 'International sale Report.csv'...\n",
      "Carregando dados para a tabela: 'cln_international_sale_report'...\n",
      "✅ Sucesso! Tabela 'cln_international_sale_report' carregada/atualizada no banco de dados.\n",
      "✅ Sucesso! Arquivo limpo salvo em: 'C:\\Users\\wallace.magalhaes\\Desktop\\E-commerce-database\\data\\clean\\cln_international_sale_report.csv'\n",
      "\n",
      "Processando 'May-2022.csv'...\n",
      "Carregando dados para a tabela: 'cln_may_2022'...\n",
      "✅ Sucesso! Tabela 'cln_may_2022' carregada/atualizada no banco de dados.\n",
      "✅ Sucesso! Arquivo limpo salvo em: 'C:\\Users\\wallace.magalhaes\\Desktop\\E-commerce-database\\data\\clean\\cln_may_2022.csv'\n",
      "\n",
      "Processando 'PL March 2021.csv'...\n",
      "Carregando dados para a tabela: 'cln_pl_march_2021'...\n",
      "✅ Sucesso! Tabela 'cln_pl_march_2021' carregada/atualizada no banco de dados.\n",
      "✅ Sucesso! Arquivo limpo salvo em: 'C:\\Users\\wallace.magalhaes\\Desktop\\E-commerce-database\\data\\clean\\cln_pl_march_2021.csv'\n",
      "\n",
      "Processando 'Sale Report.csv'...\n",
      "Carregando dados para a tabela: 'cln_sale_report'...\n",
      "✅ Sucesso! Tabela 'cln_sale_report' carregada/atualizada no banco de dados.\n",
      "✅ Sucesso! Arquivo limpo salvo em: 'C:\\Users\\wallace.magalhaes\\Desktop\\E-commerce-database\\data\\clean\\cln_sale_report.csv'\n",
      "\n",
      "==================================================\n",
      "Processo de ETL concluído.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from sqlalchemy import create_engine\n",
    "import numpy as np\n",
    "\n",
    "RAW_DATA_PATH = Path(r\"C:\\Users\\wallace.magalhaes\\Desktop\\E-commerce-database\\data\\raw\")\n",
    "CLEAN_DATA_PATH = Path(r\"C:\\Users\\wallace.magalhaes\\Desktop\\E-commerce-database\\data\\clean\")\n",
    "DB_ENGINE = create_engine(\"postgresql+psycopg2://postgres:123456@localhost:5432/ecommerce\")\n",
    "\n",
    "FILE_CONFIG = {\n",
    "    \"Amazon Sale Report.csv\": {\n",
    "        \"columns\": ['Order ID', 'Date', 'Status', 'Fulfilment', 'Sales Channel', 'SKU', 'Category', 'Qty', 'Amount'],\n",
    "    },\n",
    "    \"Cloud Warehouse Compersion Chart.csv\": {\n",
    "        \"columns\": ['Shiprocket', 'INCREFF'],\n",
    "    },\n",
    "    \"Expense IIGF.csv\": {\n",
    "        \"columns\": ['Recived Amount', 'Expance'],\n",
    "        \"date_cols\": [\"Recived Amount\"]\n",
    "    },\n",
    "    \"International sale Report.csv\": {\n",
    "        \"columns\": ['Months', 'Style', 'SKU', 'Size', 'PCS', 'RATE', 'GROSS AMT'],\n",
    "        \"date_cols\": [\"Months\"],\n",
    "        \"date_format\": \"%b-%y\"\n",
    "    },\n",
    "    \"May-2022.csv\": {\n",
    "        \"columns\": ['Catalog', 'Category', 'Weight', 'TP', 'MRP Old', 'Final MRP Old', 'Ajio MRP', 'Amazon MRP', 'Amazon FBA MRP'],\n",
    "    },\n",
    "    \"PL March 2021.csv\": { # Assumindo que você corrigiu o nome do arquivo para este\n",
    "        \"columns\": ['Catalog', 'Category', 'Weight', 'TP 1', 'TP 2', 'MRP Old', 'Final MRP Old', 'Ajio MRP', 'Amazon MRP', 'Amazon FBA MRP'],\n",
    "    },\n",
    "    \"Sale Report.csv\": {\n",
    "        \"columns\": ['SKU Code', 'Design No.', 'Stock', 'Category', 'Size', 'Color']\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "# =============================================================\n",
    "# FUNÇÕES DE TRANSFORMAÇÃO\n",
    "# =============================================================\n",
    "def preencher_dados_faltantes(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if 'qty' not in df.columns:\n",
    "        df['qty'] = 1\n",
    "    if 'amount' not in df.columns and 'gross_amt' not in df.columns:\n",
    "        df['amount'] = np.round(np.random.uniform(10, 500, size=len(df)), 2)\n",
    "    return df\n",
    "\n",
    "def padronizar_nomes_colunas(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df.columns = [col.strip().lower().replace(\" \", \"_\").replace(\"-\", \"_\") for col in df.columns]\n",
    "    return df\n",
    "\n",
    "def converter_datas(df: pd.DataFrame, config: dict) -> pd.DataFrame:\n",
    "    date_cols = config.get(\"date_cols\", [])\n",
    "    date_format = config.get(\"date_format\")\n",
    "    for col_name in date_cols:\n",
    "        col_snake_case = col_name.strip().lower().replace(\" \", \"_\").replace(\"-\", \"_\")\n",
    "        if col_snake_case in df.columns:\n",
    "            df[col_snake_case] = pd.to_datetime(df[col_snake_case], format=date_format, errors='coerce')\n",
    "    return df\n",
    "\n",
    "\n",
    "def processar_arquivo(filepath: Path, config: dict) -> pd.DataFrame:\n",
    "    print(f\"Processando '{filepath.name}'...\")\n",
    "    df = pd.read_csv(filepath, low_memory=False)\n",
    "    colunas_originais = [col.strip() for col in config[\"columns\"]]\n",
    "    df.columns = [col.strip() for col in df.columns]\n",
    "    df_filtrado = df[colunas_originais].copy()\n",
    "    \n",
    "    df_processado = (\n",
    "        df_filtrado.pipe(padronizar_nomes_colunas)\n",
    "                   .pipe(preencher_dados_faltantes)\n",
    "                   .pipe(converter_datas, config=config)\n",
    "    )\n",
    "    return df_processado\n",
    "\n",
    "# =============================================================\n",
    "# FUNÇÃO PRINCIPAL (MAIN)\n",
    "# =============================================================\n",
    "def main():\n",
    "    warnings.filterwarnings('ignore', category=UserWarning)\n",
    "    CLEAN_DATA_PATH.mkdir(exist_ok=True)\n",
    "    print(\"Iniciando processo de ETL...\\n\" + \"=\"*50)\n",
    "    \n",
    "    for filename, config in FILE_CONFIG.items():\n",
    "        try:\n",
    "            source_path = RAW_DATA_PATH / filename\n",
    "            cleaned_df = processar_arquivo(source_path, config)\n",
    "            \n",
    "            base_name = source_path.stem\n",
    "            standard_name = base_name.lower().replace(' ', '_').replace('-', '_')\n",
    "            table_name = f\"cln_{standard_name}\"\n",
    "\n",
    "            print(f\"Carregando dados para a tabela: '{table_name}'...\")\n",
    "            cleaned_df.to_sql(table_name, con=DB_ENGINE, if_exists='replace', index=False)\n",
    "            print(f\"✅ Sucesso! Tabela '{table_name}' carregada/atualizada no banco de dados.\")\n",
    "\n",
    "            destination_filename = f\"{table_name}.csv\"\n",
    "            destination_path = CLEAN_DATA_PATH / destination_filename\n",
    "            cleaned_df.to_csv(destination_path, index=False, encoding='utf-8')\n",
    "            print(f\"✅ Sucesso! Arquivo limpo salvo em: '{destination_path}'\\n\")\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"❌ ERRO: Arquivo '{filename}' não foi encontrado. Verifique a pasta e a configuração.\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Erro inesperado ao processar '{filename}': {e}\\n\")\n",
    "            \n",
    "    print(\"=\"*50 + \"\\nProcesso de ETL concluído.\")\n",
    "\n",
    "# =============================================================\n",
    "# PONTO DE ENTRADA DO SCRIPT\n",
    "# =============================================================\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f43f6f-6750-46dc-9ff1-eb370bf6bb8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
